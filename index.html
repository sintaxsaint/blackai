<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>BlackAI - Free AI Assistant | Text & Vision</title>
  <meta name="description" content="BlackAI - Free AI assistant with text chat and image analysis. Powered by Qwen 1.5B running in your browser.">
  <link rel="shortcut icon" type="image/x-icon" href="B.ico">
  <link rel="icon" type="image/x-icon" href="B.ico">
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    .loading-dots::after {
      content: '';
      animation: dots 1.5s steps(4, end) infinite;
    }
    @keyframes dots {
      0%, 20% { content: ''; }
      40% { content: '.'; }
      60% { content: '..'; }
      80%, 100% { content: '...'; }
    }
    .error-detail {
      font-family: monospace;
      font-size: 11px;
      background: #1a1a1a;
      padding: 8px;
      border-radius: 4px;
      margin-top: 8px;
      max-height: 200px;
      overflow-y: auto;
    }
  </style>
</head>
<body class="bg-gray-900 text-white h-screen flex flex-col">
  <div class="p-4 border-b border-gray-700 flex items-center justify-between">
    <div class="flex items-center gap-3">
      <img src="B.ico" alt="BlackAI" class="w-10 h-10">
      <div>
        <h1 class="text-2xl font-bold">BlackAI</h1>
        <p class="text-sm text-gray-400">Browser AI - Qwen 1.5B</p>
      </div>
    </div>
    <div class="text-xs text-right">
      <div id="modelStatus" class="text-yellow-400 loading-dots">Initializing</div>
      <div id="debugInfo" class="text-gray-500 mt-1"></div>
    </div>
  </div>

  <div id="chat" class="flex-1 overflow-y-auto p-4 space-y-3">
    <div class="text-center text-gray-400 py-8">
      <p class="mb-2">ðŸ’¬ Ask me anything or upload an image for analysis</p>
      <p class="text-sm mb-2">Powered by Qwen2.5-1.5B running in your browser</p>
      <div class="text-xs text-gray-500 mt-4 space-y-1">
        <p>âœ¨ Image analysis with ViT-GPT2</p>
        <p>ðŸ§  Text chat with Qwen 1.5B</p>
        <p>âš¡ For advanced AI, download BlackAI Desktop (7B/32B models)</p>
      </div>
    </div>
  </div>

  <div class="p-4 border-t border-gray-700">
    <div id="imagePreview" class="hidden mb-2 relative inline-block">
      <img id="previewImg" class="max-h-32 rounded-lg border border-gray-600" />
      <button onclick="clearImage()" class="absolute -top-2 -right-2 bg-red-600 hover:bg-red-700 text-white rounded-full w-6 h-6 flex items-center justify-center text-sm font-bold">&times;</button>
    </div>
    <div class="flex gap-2">
      <input type="file" id="imageInput" accept="image/*" class="hidden" onchange="handleImageSelect(event)" />
      <button id="imgBtn" class="bg-gray-700 hover:bg-gray-600 px-3 py-3 rounded-lg transition-colors" onclick="document.getElementById('imageInput').click()" title="Upload an image">
        <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
          <path stroke-linecap="round" stroke-linejoin="round" d="M3 7h2l2-3h10l2 3h2a1 1 0 011 1v11a1 1 0 01-1 1H3a1 1 0 01-1-1V8a1 1 0 011-1z" />
          <circle cx="12" cy="13" r="4" />
        </svg>
      </button>
      <input id="input" class="flex-1 bg-gray-800 px-4 py-3 rounded-lg outline-none focus:ring-2 focus:ring-green-500" placeholder="Ask me anything..." onkeypress="if(event.key === 'Enter') sendMessage()" />
      <button id="send" class="bg-green-600 hover:bg-green-700 px-6 py-3 rounded-lg font-medium transition-colors disabled:opacity-50 disabled:cursor-not-allowed" onclick="sendMessage()">Send</button>
    </div>
    <div id="status" class="text-xs text-gray-500 mt-2"></div>
  </div>

  <script type="module">
    console.log("[BlackAI] Initializing browser version...");

    let visionModel = null;
    let textModel = null;
    let isLoading = false;
    let pendingImage = null;
    let history = [];
    let modelsReady = false;

    const chat = document.getElementById("chat");
    const input = document.getElementById("input");
    const sendBtn = document.getElementById("send");
    const status = document.getElementById("status");
    const imagePreview = document.getElementById("imagePreview");
    const previewImg = document.getElementById("previewImg");
    const modelStatus = document.getElementById("modelStatus");
    const debugInfo = document.getElementById("debugInfo");

    function logError(context, error) {
      console.error(`[BlackAI Error - ${context}]:`, error);
      debugInfo.textContent = `Error: ${context}`;
      history.push({
        role: "error",
        content: `Error in ${context}:\n${error.message}`,
        detail: error.stack || error.toString()
      });
      render();
    }

    async function loadModels() {
      try {
        console.log("[BlackAI] Loading Transformers.js...");
        modelStatus.textContent = "Loading AI...";
        
        const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2');
        env.allowLocalModels = false;
        
        modelStatus.textContent = "Loading models (~500MB first time)...";
        debugInfo.textContent = "Downloading...";

        console.log("[BlackAI] Loading vision model...");
        visionModel = await pipeline('image-to-text', 'Xenova/vit-gpt2-image-captioning');
        console.log("[BlackAI] Vision model âœ“");

        console.log("[BlackAI] Loading Qwen 1.5B text model...");
        textModel = await pipeline('text-generation', 'Xenova/Qwen2.5-1.5B-Instruct', {
          dtype: 'q8'  // 8-bit quantization for browser
        });
        console.log("[BlackAI] Text model âœ“");

        modelsReady = true;
        modelStatus.textContent = "âœ“ Ready";
        modelStatus.className = "text-green-400";
        debugInfo.textContent = "Qwen 1.5B loaded";
        setStatus("Ready! Ask me anything.");
        
        console.log("[BlackAI] All models loaded!");

      } catch (err) {
        logError("Model Loading", err);
        modelStatus.textContent = "âŒ Load Failed";
        modelStatus.className = "text-red-400";
        setStatus("Failed to load. Refresh page.");
      }
    }

    loadModels();

    window.handleImageSelect = function(event) {
      try {
        const file = event.target.files[0];
        if (!file) return;
        if (file.size > 10 * 1024 * 1024) {
          alert("Image too large. Max 10MB.");
          return;
        }
        const reader = new FileReader();
        reader.onload = function(e) {
          pendingImage = e.target.result;
          previewImg.src = pendingImage;
          imagePreview.classList.remove("hidden");
          console.log("[BlackAI] Image loaded");
        };
        reader.onerror = (err) => logError("Image Reading", err);
        reader.readAsDataURL(file);
      } catch (err) {
        logError("Image Selection", err);
      }
    };

    window.clearImage = function() {
      pendingImage = null;
      imagePreview.classList.add("hidden");
      previewImg.src = "";
      document.getElementById("imageInput").value = "";
    };

    function render() {
      if (history.length === 0) {
        chat.innerHTML = `
          <div class="text-center text-gray-400 py-8">
            <p class="mb-2">ðŸ’¬ Ask me anything or upload an image for analysis</p>
            <p class="text-sm mb-2">Powered by Qwen2.5-1.5B running in your browser</p>
            <div class="text-xs text-gray-500 mt-4 space-y-1">
              <p>âœ¨ Image analysis with ViT-GPT2</p>
              <p>ðŸ§  Text chat with Qwen 1.5B</p>
              <p>âš¡ For advanced AI, download BlackAI Desktop (7B/32B models)</p>
            </div>
          </div>
        `;
        return;
      }

      chat.innerHTML = history.map(m => {
        const isUser = m.role === 'user';
        const isError = m.role === 'error';
        const bgClass = isUser ? 'bg-green-600' : isError ? 'bg-red-600' : 'bg-gray-800';
        const label = isUser ? 'You' : isError ? 'Error' : 'BlackAI';
        let imageHtml = '';
        if (m.image) {
          imageHtml = `<img src="${m.image}" class="max-h-48 rounded mb-2 border border-gray-600" />`;
        }
        let detailHtml = '';
        if (m.detail && isError) {
          detailHtml = `<div class="error-detail">${escapeHtml(m.detail)}</div>`;
        }
        return `
          <div class="${isUser ? 'text-right' : 'text-left'}">
            <div class="inline-block max-w-2xl px-4 py-3 rounded-lg ${bgClass} text-white">
              <div class="text-xs opacity-75 mb-1">${label}</div>
              ${imageHtml}
              <div class="whitespace-pre-wrap">${escapeHtml(m.content)}</div>
              ${detailHtml}
            </div>
          </div>
        `;
      }).join("");
      chat.scrollTop = chat.scrollHeight;
    }

    function escapeHtml(text) {
      const div = document.createElement('div');
      div.textContent = text;
      return div.innerHTML;
    }

    function setStatus(msg) {
      status.textContent = msg;
      console.log(`[BlackAI] ${msg}`);
    }

    async function analyzeImage(imageUrl) {
      if (!visionModel) throw new Error("Vision model not loaded");
      console.log("[BlackAI] Analyzing image...");
      const { RawImage } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2');
      const response = await fetch(imageUrl);
      const blob = await response.blob();
      const image = await RawImage.read(blob);
      const result = await visionModel(image);
      return result[0].generated_text;
    }

    async function generateText(prompt) {
      if (!textModel) throw new Error("Text model not loaded");
      console.log("[BlackAI] Generating response...");
      
      // Format for Qwen chat template
      const messages = [
        { role: "system", content: "You are BlackAI, a helpful assistant. Give brief, clear answers." },
        { role: "user", content: prompt }
      ];
      
      // Note: Transformers.js doesn't support chat templates yet, so we format manually
      const formattedPrompt = `<|im_start|>system\nYou are BlackAI, a helpful assistant.<|im_end|>\n<|im_start|>user\n${prompt}<|im_end|>\n<|im_start|>assistant\n`;
      
      const result = await textModel(formattedPrompt, {
        max_new_tokens: 200,
        temperature: 0.7,
        top_p: 0.9,
        do_sample: true
      });

      let response = result[0].generated_text;
      
      // Extract assistant response
      if (response.includes('<|im_start|>assistant\n')) {
        response = response.split('<|im_start|>assistant\n').pop();
      }
      response = response.split('<|im_end|>')[0].trim();
      
      return response || "Sorry, I couldn't generate a good response. Try rephrasing?";
    }

    window.sendMessage = async function() {
      try {
        const text = input.value.trim();
        if (!text && !pendingImage) return;
        if (isLoading) return;
        if (!modelsReady) {
          setStatus("Wait for models to load...");
          return;
        }

        console.log("[BlackAI] Processing message...");
        history.push({ role: "user", content: text || "(image)", image: pendingImage });
        const sentImage = pendingImage;
        input.value = "";
        clearImage();
        render();

        isLoading = true;
        sendBtn.disabled = true;

        let reply = "";

        if (sentImage) {
          setStatus("Analyzing image...");
          const analysis = await analyzeImage(sentImage);
          
          if (text) {
            reply = `I can see: ${analysis}\n\n`;
            try {
              const answer = await generateText(`Based on seeing "${analysis}" in an image, ${text}`);
              reply += answer;
            } catch (err) {
              console.warn("[BlackAI] Text gen failed:", err);
              reply += "For detailed analysis, use BlackAI Desktop.";
            }
          } else {
            reply = analysis;
          }
        } else if (text) {
          setStatus("Thinking...");
          reply = await generateText(text);
        }

        history.push({ role: "assistant", content: reply });
        render();
        setStatus("");
        console.log("[BlackAI] Done!");

      } catch (err) {
        logError("Send Message", err);
        setStatus("Error - see above");
      } finally {
        isLoading = false;
        sendBtn.disabled = false;
      }
    };

    render();
    console.log("[BlackAI] Ready and waiting...");
  </script>
</body>
</html>
