<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Basic Meta Tags -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>BlackAI - Free General AI Assistant | Text & Vision</title>
  <meta name="description" content="BlackAI is a free general-purpose AI assistant. Chat, analyze images, get homework help, and more - all powered by advanced AI.">
  <meta name="keywords" content="AI assistant, chatbot, image analysis, BlackAI, free AI, homework help, general AI">
  <meta name="author" content="BlackAI">
  <meta name="robots" content="index, follow">

  <!-- Favicon -->
  <link rel="shortcut icon" type="image/x-icon" href="B.ico">
  <link rel="icon" type="image/x-icon" href="B.ico">

  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>

  <style>
    .loading-dots::after {
      content: '';
      animation: dots 1.5s steps(4, end) infinite;
    }
    @keyframes dots {
      0%, 20% { content: ''; }
      40% { content: '.'; }
      60% { content: '..'; }
      80%, 100% { content: '...'; }
    }
    .error-detail {
      font-family: monospace;
      font-size: 11px;
      background: #1a1a1a;
      padding: 8px;
      border-radius: 4px;
      margin-top: 8px;
      max-height: 200px;
      overflow-y: auto;
    }
  </style>
</head>
<body class="bg-gray-900 text-white h-screen flex flex-col">
  <!-- Header -->
  <div class="p-4 border-b border-gray-700 flex items-center justify-between">
    <div class="flex items-center gap-3">
      <img src="B.ico" alt="BlackAI" class="w-10 h-10">
      <div>
        <h1 class="text-2xl font-bold">BlackAI</h1>
        <p class="text-sm text-gray-400">General AI Assistant</p>
      </div>
    </div>
    <div class="text-xs text-right">
      <div id="modelStatus" class="text-yellow-400 loading-dots">Initializing</div>
      <div id="debugInfo" class="text-gray-500 mt-1"></div>
    </div>
  </div>

  <!-- Chat Area -->
  <div id="chat" class="flex-1 overflow-y-auto p-4 space-y-3">
    <div class="text-center text-gray-400 py-8">
      <p class="mb-2">ðŸ’¬ Ask me anything - I can help with questions, homework, analysis, and more</p>
      <p class="text-sm mb-2">ðŸ“¸ Upload images for visual analysis â€¢ ðŸ’­ Or just chat normally</p>
      <div class="text-xs text-gray-500 mt-4">
        <p>âœ¨ Powered by browser-based AI models</p>
      </div>
    </div>
  </div>

  <!-- Input Area -->
  <div class="p-4 border-t border-gray-700">
    <!-- Image Preview -->
    <div id="imagePreview" class="hidden mb-2 relative inline-block">
      <img id="previewImg" class="max-h-32 rounded-lg border border-gray-600" />
      <button onclick="clearImage()" class="absolute -top-2 -right-2 bg-red-600 hover:bg-red-700 text-white rounded-full w-6 h-6 flex items-center justify-center text-sm font-bold">&times;</button>
    </div>
    <div class="flex gap-2">
      <input type="file" id="imageInput" accept="image/*" class="hidden" onchange="handleImageSelect(event)" />
      <button
        id="imgBtn"
        class="bg-gray-700 hover:bg-gray-600 px-3 py-3 rounded-lg transition-colors"
        onclick="document.getElementById('imageInput').click()"
        title="Upload an image"
      >
        <svg xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
          <path stroke-linecap="round" stroke-linejoin="round" d="M3 7h2l2-3h10l2 3h2a1 1 0 011 1v11a1 1 0 01-1 1H3a1 1 0 01-1-1V8a1 1 0 011-1z" />
          <circle cx="12" cy="13" r="4" />
        </svg>
      </button>
      <input
        id="input"
        class="flex-1 bg-gray-800 px-4 py-3 rounded-lg outline-none focus:ring-2 focus:ring-green-500"
        placeholder="Ask me anything or upload an image..."
        onkeypress="if(event.key === 'Enter') sendMessage()"
      />
      <button
        id="send"
        class="bg-green-600 hover:bg-green-700 px-6 py-3 rounded-lg font-medium transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
        onclick="sendMessage()"
      >
        Send
      </button>
    </div>
    <div id="status" class="text-xs text-gray-500 mt-2"></div>
  </div>

  <script type="module">
    console.log("[BlackAI] Starting initialization...");

    // ==== GLOBAL STATE ==== //
    let visionModel = null;
    let textModel = null;
    let isLoading = false;
    let pendingImage = null;
    let history = [];
    let modelsReady = false;

    // ==== DOM ELEMENTS ==== //
    const chat = document.getElementById("chat");
    const input = document.getElementById("input");
    const sendBtn = document.getElementById("send");
    const status = document.getElementById("status");
    const imagePreview = document.getElementById("imagePreview");
    const previewImg = document.getElementById("previewImg");
    const modelStatus = document.getElementById("modelStatus");
    const debugInfo = document.getElementById("debugInfo");

    // ==== ERROR LOGGING ==== //
    function logError(context, error) {
      console.error(`[BlackAI Error - ${context}]:`, error);
      debugInfo.textContent = `Last error: ${context}`;
      
      // Show detailed error in chat
      const errorMsg = {
        role: "error",
        content: `Error in ${context}:\n${error.message}`,
        detail: error.stack || error.toString()
      };
      history.push(errorMsg);
      render();
    }

    // ==== LOAD MODELS ==== //
    async function loadModels() {
      try {
        console.log("[BlackAI] Loading Transformers.js...");
        modelStatus.textContent = "Loading AI libraries...";
        
        // Import transformers.js
        const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2');
        env.allowLocalModels = false;
        
        console.log("[BlackAI] Transformers.js loaded, loading models...");
        modelStatus.textContent = "Loading AI models (1-2 min first time)...";
        debugInfo.textContent = "Downloading models...";

        // Load vision model
        console.log("[BlackAI] Loading vision model...");
        visionModel = await pipeline('image-to-text', 'Xenova/vit-gpt2-image-captioning');
        console.log("[BlackAI] Vision model loaded âœ“");

        // Load text model - using GPT-2 (small, reliable, browser-safe)
        console.log("[BlackAI] Loading text model...");
        textModel = await pipeline('text-generation', 'Xenova/distilgpt2');
        console.log("[BlackAI] Text model loaded âœ“");

        modelsReady = true;
        modelStatus.textContent = "âœ“ Ready";
        modelStatus.className = "text-green-400";
        debugInfo.textContent = "Models loaded successfully";
        setStatus("Ready! Ask me anything or upload an image.");
        
        console.log("[BlackAI] All models loaded successfully!");

      } catch (err) {
        logError("Model Loading", err);
        modelStatus.textContent = "âŒ Load Failed";
        modelStatus.className = "text-red-400";
        setStatus("Failed to load AI models. See error above.");
      }
    }

    // Start loading immediately
    loadModels();

    // ==== IMAGE HANDLING ==== //
    window.handleImageSelect = function(event) {
      try {
        const file = event.target.files[0];
        if (!file) return;

        if (file.size > 10 * 1024 * 1024) {
          alert("Image too large. Please use an image under 10MB.");
          return;
        }

        const reader = new FileReader();
        reader.onload = function(e) {
          pendingImage = e.target.result;
          previewImg.src = pendingImage;
          imagePreview.classList.remove("hidden");
          console.log("[BlackAI] Image loaded for upload");
        };
        reader.onerror = function(err) {
          logError("Image Reading", err);
        };
        reader.readAsDataURL(file);
      } catch (err) {
        logError("Image Selection", err);
      }
    };

    window.clearImage = function() {
      pendingImage = null;
      imagePreview.classList.add("hidden");
      previewImg.src = "";
      document.getElementById("imageInput").value = "";
    };

    // ==== RENDER CHAT ==== //
    function render() {
      if (history.length === 0) {
        chat.innerHTML = `
          <div class="text-center text-gray-400 py-8">
            <p class="mb-2">ðŸ’¬ Ask me anything - I can help with questions, homework, analysis, and more</p>
            <p class="text-sm mb-2">ðŸ“¸ Upload images for visual analysis â€¢ ðŸ’­ Or just chat normally</p>
            <div class="text-xs text-gray-500 mt-4">
              <p>âœ¨ Powered by browser-based AI models</p>
            </div>
          </div>
        `;
        return;
      }

      chat.innerHTML = history.map(m => {
        const isUser = m.role === 'user';
        const isError = m.role === 'error';
        const bgClass = isUser ? 'bg-green-600 text-white'
                      : isError ? 'bg-red-600 text-white'
                      : 'bg-gray-800 text-gray-100';
        const label = isUser ? 'You' : isError ? 'Error' : 'BlackAI';

        let imageHtml = '';
        if (m.image) {
          imageHtml = `<img src="${m.image}" class="max-h-48 rounded mb-2 border border-gray-600" alt="Uploaded image" />`;
        }

        let detailHtml = '';
        if (m.detail && isError) {
          detailHtml = `<div class="error-detail">${escapeHtml(m.detail)}</div>`;
        }

        return `
          <div class="${isUser ? 'text-right' : 'text-left'}">
            <div class="inline-block max-w-2xl px-4 py-3 rounded-lg ${bgClass}">
              <div class="text-xs opacity-75 mb-1">${label}</div>
              ${imageHtml}
              <div class="whitespace-pre-wrap">${escapeHtml(m.content)}</div>
              ${detailHtml}
            </div>
          </div>
        `;
      }).join("");
      chat.scrollTop = chat.scrollHeight;
    }

    // ==== ESCAPE HTML ==== //
    function escapeHtml(text) {
      const div = document.createElement('div');
      div.textContent = text;
      return div.innerHTML;
    }

    // ==== UPDATE STATUS ==== //
    function setStatus(msg) {
      status.textContent = msg;
      console.log(`[BlackAI Status] ${msg}`);
    }

    // ==== ANALYZE IMAGE ==== //
    async function analyzeImage(imageUrl) {
      if (!visionModel) {
        throw new Error("Vision model not loaded yet");
      }

      console.log("[BlackAI] Analyzing image...");
      
      try {
        // Create image element from data URL
        const img = new Image();
        img.crossOrigin = "anonymous";
        img.src = imageUrl;
        
        // Wait for image to load
        await new Promise((resolve, reject) => {
          img.onload = resolve;
          img.onerror = reject;
        });
        
        console.log("[BlackAI] Image loaded, running inference...");
        
        // Pass the image element directly (not URL)
        const result = await visionModel(img);
        
        console.log("[BlackAI] Analysis result:", result);
        
        return result[0].generated_text;
        
      } catch (err) {
        console.error("[BlackAI] Image analysis error:", err);
        throw new Error(`Image analysis failed: ${err.message}`);
      }
    }

    // ==== GENERATE TEXT ==== //
    async function generateText(prompt) {
      if (!textModel) {
        throw new Error("Text model not loaded yet");
      }

      console.log("[BlackAI] Generating text response...");
      
      // Check for simple patterns first (faster and more reliable)
      const lowerPrompt = prompt.toLowerCase();
      
      // Math questions
      if (lowerPrompt.includes('what is') || lowerPrompt.includes('what\'s')) {
        const mathMatch = lowerPrompt.match(/(\d+)\s*[\+\s]+\s*(\d+)/);
        if (mathMatch) {
          const result = parseInt(mathMatch[1]) + parseInt(mathMatch[2]);
          return `${mathMatch[1]} + ${mathMatch[2]} = ${result}`;
        }
        
        const subMatch = lowerPrompt.match(/(\d+)\s*-\s*(\d+)/);
        if (subMatch) {
          const result = parseInt(subMatch[1]) - parseInt(subMatch[2]);
          return `${subMatch[1]} - ${subMatch[2]} = ${result}`;
        }
        
        const multMatch = lowerPrompt.match(/(\d+)\s*[\*xÃ—]\s*(\d+)/);
        if (multMatch) {
          const result = parseInt(multMatch[1]) * parseInt(multMatch[2]);
          return `${multMatch[1]} Ã— ${multMatch[2]} = ${result}`;
        }
      }
      
      // Greetings
      if (/^(hi|hello|hey|sup|yo|wassup|what'?s up)/i.test(lowerPrompt)) {
        return "Hey! I'm BlackAI. I can help with questions, analyze images, and more. What do you need?";
      }
      
      // How are you
      if (/how are you|you good|you ok/i.test(lowerPrompt)) {
        return "I'm working great! Ready to help you with whatever you need. ðŸ˜Š";
      }
      
      // About the logo
      if (/logo|icon|image of you/i.test(lowerPrompt)) {
        return "My logo is the letter 'B' - stands for Black ecosystem! Simple and clean. ðŸ˜Ž";
      }
      
      // How do you work
      if (/how (do|does) (you|this|it) work/i.test(lowerPrompt)) {
        return "I'm BlackAI, running in your browser! I use AI models for text and vision analysis. For simple questions I use quick pattern matching, and for complex stuff I use GPT-2. Want to know more about a specific feature?";
      }
      
      // What can you do
      if (/what can you|what do you do|your capabilities|your features/i.test(lowerPrompt)) {
        return "I can:\nâ€¢ Answer questions (basic level)\nâ€¢ Analyze images you upload\nâ€¢ Do simple math\nâ€¢ Have conversations\n\nFor advanced AI, download BlackAI Desktop with Qwen-7B!";
      }
      
      // For everything else, use GPT-2 with careful prompting
      const formattedPrompt = `Q: ${prompt}\nA:`;
      
      try {
        const result = await textModel(formattedPrompt, {
          max_new_tokens: 50,
          temperature: 0.7,
          top_p: 0.9,
          repetition_penalty: 1.3,
          do_sample: true
        });

        let response = result[0].generated_text;
        
        // Extract answer part
        if (response.includes('A:')) {
          const parts = response.split('A:');
          response = parts[parts.length - 1].trim();
        }
        
        // Remove the original prompt
        response = response.replace(formattedPrompt, '').trim();
        
        // Stop at double newline or new question
        const stopPoint = response.search(/\n\n|Q:|Question:/);
        if (stopPoint > 0) {
          response = response.substring(0, stopPoint).trim();
        }
        
        // Clean up repetition
        const sentences = response.split('. ');
        if (sentences.length > 3) {
          response = sentences.slice(0, 2).join('. ') + '.';
        }
        
        // Validation
        if (response.length < 10 || response.length > 300 || response.includes('undefined')) {
          return "I'm the basic browser version of BlackAI. For complex questions, download BlackAI Desktop with Qwen-7B for much better answers!";
        }

        return response;
        
      } catch (err) {
        console.warn("[BlackAI] GPT-2 generation failed:", err);
        return "I'm having trouble with that question. This is the browser version - try BlackAI Desktop for better AI responses!";
      }
    }

    // ==== SEND MESSAGE ==== //
    window.sendMessage = async function() {
      try {
        const text = input.value.trim();
        if (!text && !pendingImage) return;
        if (isLoading) return;
        
        if (!modelsReady) {
          setStatus("Please wait for models to load...");
          return;
        }

        console.log("[BlackAI] Sending message:", text ? text.substring(0, 50) : "(image only)");

        // Add user message
        const userMsg = { role: "user", content: text || "(image)" };
        if (pendingImage) userMsg.image = pendingImage;
        history.push(userMsg);

        const sentImage = pendingImage;
        input.value = "";
        clearImage();
        render();

        isLoading = true;
        sendBtn.disabled = true;
        setStatus(sentImage ? "Analyzing image..." : "Thinking...");

        let reply = "";

        if (sentImage) {
          // Image analysis mode
          const analysis = await analyzeImage(sentImage);
          
          if (text) {
            // User asked a question about the image
            reply = `I can see: ${analysis}\n\n`;
            
            // Try to answer the question
            try {
              const answer = await generateText(`Based on this image showing "${analysis}", ${text}`);
              reply += `Regarding your question: ${answer}\n\n`;
            } catch (err) {
              console.warn("[BlackAI] Text generation failed:", err);
              reply += `Regarding "${text}": This is the browser version with limited Q&A. For detailed questions, use BlackAI Desktop.\n\n`;
            }
            
            reply += "ðŸ’¡ For advanced image understanding, try BlackAI Desktop.";
          } else {
            // Just describe the image
            reply = analysis;
          }
        } else {
          // Text-only conversation
          try {
            reply = await generateText(text);
            
            // Add helpful note for complex questions
            if (text.length > 100) {
              reply += "\n\nðŸ’¡ For more detailed answers, try BlackAI Desktop with Qwen-7B.";
            }
          } catch (err) {
            logError("Text Generation", err);
            throw err;
          }
        }

        history.push({ role: "assistant", content: reply });
        render();
        setStatus("");

        console.log("[BlackAI] Response generated successfully");

      } catch (err) {
        logError("Send Message", err);
        setStatus("Error occurred - see message above");
      } finally {
        isLoading = false;
        sendBtn.disabled = false;
      }
    };

    // ==== INITIAL RENDER ==== //
    render();
    
    console.log("[BlackAI] Initialization complete, waiting for models...");
  </script>
</body>
</html>
